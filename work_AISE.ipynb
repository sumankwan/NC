{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading weights from group_models/group2/VGG19_weights.pth: Error(s) in loading state_dict for VGG:\n",
      "\tMissing key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.1.weight\", \"features.1.bias\", \"features.1.running_mean\", \"features.1.running_var\", \"features.3.weight\", \"features.3.bias\", \"features.4.weight\", \"features.4.bias\", \"features.4.running_mean\", \"features.4.running_var\", \"features.7.weight\", \"features.7.bias\", \"features.8.weight\", \"features.8.bias\", \"features.8.running_mean\", \"features.8.running_var\", \"features.10.weight\", \"features.10.bias\", \"features.11.weight\", \"features.11.bias\", \"features.11.running_mean\", \"features.11.running_var\", \"features.14.weight\", \"features.14.bias\", \"features.15.weight\", \"features.15.bias\", \"features.15.running_mean\", \"features.15.running_var\", \"features.17.weight\", \"features.17.bias\", \"features.18.weight\", \"features.18.bias\", \"features.18.running_mean\", \"features.18.running_var\", \"features.20.weight\", \"features.20.bias\", \"features.21.weight\", \"features.21.bias\", \"features.21.running_mean\", \"features.21.running_var\", \"features.23.weight\", \"features.23.bias\", \"features.24.weight\", \"features.24.bias\", \"features.24.running_mean\", \"features.24.running_var\", \"features.27.weight\", \"features.27.bias\", \"features.28.weight\", \"features.28.bias\", \"features.28.running_mean\", \"features.28.running_var\", \"features.30.weight\", \"features.30.bias\", \"features.31.weight\", \"features.31.bias\", \"features.31.running_mean\", \"features.31.running_var\", \"features.33.weight\", \"features.33.bias\", \"features.34.weight\", \"features.34.bias\", \"features.34.running_mean\", \"features.34.running_var\", \"features.36.weight\", \"features.36.bias\", \"features.37.weight\", \"features.37.bias\", \"features.37.running_mean\", \"features.37.running_var\", \"features.40.weight\", \"features.40.bias\", \"features.41.weight\", \"features.41.bias\", \"features.41.running_mean\", \"features.41.running_var\", \"features.43.weight\", \"features.43.bias\", \"features.44.weight\", \"features.44.bias\", \"features.44.running_mean\", \"features.44.running_var\", \"features.46.weight\", \"features.46.bias\", \"features.47.weight\", \"features.47.bias\", \"features.47.running_mean\", \"features.47.running_var\", \"features.49.weight\", \"features.49.bias\", \"features.50.weight\", \"features.50.bias\", \"features.50.running_mean\", \"features.50.running_var\", \"classifier.weight\", \"classifier.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"module.features.0.weight\", \"module.features.0.bias\", \"module.features.1.weight\", \"module.features.1.bias\", \"module.features.1.running_mean\", \"module.features.1.running_var\", \"module.features.1.num_batches_tracked\", \"module.features.3.weight\", \"module.features.3.bias\", \"module.features.4.weight\", \"module.features.4.bias\", \"module.features.4.running_mean\", \"module.features.4.running_var\", \"module.features.4.num_batches_tracked\", \"module.features.7.weight\", \"module.features.7.bias\", \"module.features.8.weight\", \"module.features.8.bias\", \"module.features.8.running_mean\", \"module.features.8.running_var\", \"module.features.8.num_batches_tracked\", \"module.features.10.weight\", \"module.features.10.bias\", \"module.features.11.weight\", \"module.features.11.bias\", \"module.features.11.running_mean\", \"module.features.11.running_var\", \"module.features.11.num_batches_tracked\", \"module.features.14.weight\", \"module.features.14.bias\", \"module.features.15.weight\", \"module.features.15.bias\", \"module.features.15.running_mean\", \"module.features.15.running_var\", \"module.features.15.num_batches_tracked\", \"module.features.17.weight\", \"module.features.17.bias\", \"module.features.18.weight\", \"module.features.18.bias\", \"module.features.18.running_mean\", \"module.features.18.running_var\", \"module.features.18.num_batches_tracked\", \"module.features.20.weight\", \"module.features.20.bias\", \"module.features.21.weight\", \"module.features.21.bias\", \"module.features.21.running_mean\", \"module.features.21.running_var\", \"module.features.21.num_batches_tracked\", \"module.features.23.weight\", \"module.features.23.bias\", \"module.features.24.weight\", \"module.features.24.bias\", \"module.features.24.running_mean\", \"module.features.24.running_var\", \"module.features.24.num_batches_tracked\", \"module.features.27.weight\", \"module.features.27.bias\", \"module.features.28.weight\", \"module.features.28.bias\", \"module.features.28.running_mean\", \"module.features.28.running_var\", \"module.features.28.num_batches_tracked\", \"module.features.30.weight\", \"module.features.30.bias\", \"module.features.31.weight\", \"module.features.31.bias\", \"module.features.31.running_mean\", \"module.features.31.running_var\", \"module.features.31.num_batches_tracked\", \"module.features.33.weight\", \"module.features.33.bias\", \"module.features.34.weight\", \"module.features.34.bias\", \"module.features.34.running_mean\", \"module.features.34.running_var\", \"module.features.34.num_batches_tracked\", \"module.features.36.weight\", \"module.features.36.bias\", \"module.features.37.weight\", \"module.features.37.bias\", \"module.features.37.running_mean\", \"module.features.37.running_var\", \"module.features.37.num_batches_tracked\", \"module.features.40.weight\", \"module.features.40.bias\", \"module.features.41.weight\", \"module.features.41.bias\", \"module.features.41.running_mean\", \"module.features.41.running_var\", \"module.features.41.num_batches_tracked\", \"module.features.43.weight\", \"module.features.43.bias\", \"module.features.44.weight\", \"module.features.44.bias\", \"module.features.44.running_mean\", \"module.features.44.running_var\", \"module.features.44.num_batches_tracked\", \"module.features.46.weight\", \"module.features.46.bias\", \"module.features.47.weight\", \"module.features.47.bias\", \"module.features.47.running_mean\", \"module.features.47.running_var\", \"module.features.47.num_batches_tracked\", \"module.features.49.weight\", \"module.features.49.bias\", \"module.features.50.weight\", \"module.features.50.bias\", \"module.features.50.running_mean\", \"module.features.50.running_var\", \"module.features.50.num_batches_tracked\", \"module.classifier.weight\", \"module.classifier.bias\". \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from group_models.group2.group2 import VGG  # Importing the VGG class from group2.py\n",
    "\n",
    "def load_vgg_model_and_weights(weight_path, vgg_type='VGG19'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = VGG(vgg_type).to(device)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "        print(f\"Successfully loaded weights from {weight_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights from {weight_path}: {str(e)}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weight_path = 'group_models/group2/VGG19_weights.pth'  # Adjust this path if your weight file is located somewhere else\n",
    "    model = load_vgg_model_and_weights(weight_path, 'VGG19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights from group_models/group2/VGG19_weights.pth.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from group_models.group2.group2 import VGG  # Importing the VGG class from group2.py\n",
    "\n",
    "def load_vgg_model_and_weights(weight_path, vgg_type='VGG19'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = VGG(vgg_type).to(device)\n",
    "\n",
    "    # Load the state dictionary and potentially strip the 'module.' prefix\n",
    "    state_dict = torch.load(weight_path, map_location=device)\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        print(f\"Successfully loaded weights from {weight_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights from {weight_path}: {str(e)}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weight_path = 'group_models/group2/VGG19_weights.pth'  # Adjust this path if your weight file is located somewhere else\n",
    "    model = load_vgg_model_and_weights(weight_path, 'VGG19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.1.0+cpu\n",
      "CUDA Version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Number of GPUs available: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_clean_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL Vostro 14\\OneDrive - Singapore Management University\\Desktop\\CS612\\Group project\\Neural-Cleanse-Pytorch\\work_AISE.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mask_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpath_to_mask_image.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),  \u001b[39m# Resize to the input size of VGG\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m clean_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(clean_image_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m mask \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(mask_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m clean_image \u001b[39m=\u001b[39m transform(clean_image)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3224\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3226\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3227\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3228\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_clean_image.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the clean image and mask\n",
    "clean_image_path = 'path_to_clean_image.jpg'\n",
    "mask_path = 'path_to_mask_image.jpg'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to the input size of VGG\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "clean_image = Image.open(clean_image_path)\n",
    "mask = Image.open(mask_path)\n",
    "\n",
    "clean_image = transform(clean_image)\n",
    "mask = transform(mask)\n",
    "\n",
    "# Overlay the mask onto the clean image\n",
    "trigger_intensity = 1.0  # You can adjust this based on the mask intensity\n",
    "overlayed_image = clean_image + trigger_intensity * mask\n",
    "overlayed_image = torch.clamp(overlayed_image, 0, 1)  # Ensure values are between 0 and 1\n",
    "\n",
    "# Convert back to PIL Image for visualization\n",
    "overlayed_image_pil = transforms.ToPILImage()(overlayed_image)\n",
    "overlayed_image_pil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:10<00:00, 2406819.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/cifar10_visualize_mask_label_6.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL Vostro 14\\OneDrive - Singapore Management University\\Desktop\\CS612\\Group project\\Neural-Cleanse-Pytorch\\work_AISE.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m current_label \u001b[39m=\u001b[39m labels[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m overlayed_image \u001b[39m=\u001b[39m apply_mask_to_clean_image(inputs[\u001b[39m0\u001b[39;49m], masks_path_template, current_label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# If you want to visualize the overlayed image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m overlayed_image_pil \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToPILImage()(overlayed_image)\n",
      "\u001b[1;32mc:\\Users\\DELL Vostro 14\\OneDrive - Singapore Management University\\Desktop\\CS612\\Group project\\Neural-Cleanse-Pytorch\\work_AISE.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_mask_to_clean_image\u001b[39m(clean_image, mask_path, label):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     mask \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(mask_path\u001b[39m.\u001b[39;49mformat(label))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     mask \u001b[39m=\u001b[39m transform(mask)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL%20Vostro%2014/OneDrive%20-%20Singapore%20Management%20University/Desktop/CS612/Group%20project/Neural-Cleanse-Pytorch/work_AISE.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     trigger_intensity \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3224\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3226\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3227\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3228\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/cifar10_visualize_mask_label_6.png'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def apply_mask_to_clean_image(clean_image, mask_path, label):\n",
    "    mask = Image.open(mask_path.format(label))\n",
    "    mask = transform(mask)\n",
    "\n",
    "    trigger_intensity = 1.0\n",
    "    overlayed_image = clean_image + trigger_intensity * mask\n",
    "    overlayed_image = torch.clamp(overlayed_image, 0, 1)\n",
    "\n",
    "    return overlayed_image\n",
    "\n",
    "masks_path_template = '../results/cifar10_visualize_mask_label_{}.png'\n",
    "\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    current_label = labels[0].item()\n",
    "\n",
    "    overlayed_image = apply_mask_to_clean_image(inputs[0], masks_path_template, current_label)\n",
    "\n",
    "    # If you want to visualize the overlayed image\n",
    "    overlayed_image_pil = transforms.ToPILImage()(overlayed_image)\n",
    "    overlayed_image_pil.show()\n",
    "    \n",
    "    # You might want to break after a few iterations to not open too many images\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ...\n",
    "\n",
    "# Inside your loop\n",
    "# If you want to visualize the overlayed image\n",
    "overlayed_image_np = overlayed_image.permute(1, 2, 0).numpy()\n",
    "plt.imshow(overlayed_image_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Transformation for CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True, num_workers=2)\n",
    "\n",
    "# Get first batch of images\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Function to show image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Display first 10 images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
